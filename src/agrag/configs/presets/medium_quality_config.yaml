shared:
  pipeline_batch_size: 20

data:
  chunk_size: 128
  chunk_overlap: 16
  file_extns:
    - "pdf"
    - "txt"

embedding:
  embedding_model: BAAI/bge-large-en
  pooling_strategy: cls
  normalize_embeddings: false
  hf_tokenizer_params: 
    truncation: true
    padding: true
  normalization_params: {'p': 2, 'dim': 1, 'eps': 1e-12}
  embedding_batch_size: 64

vector_db:
  db_type: faiss
  params: {gpu: False}
  similarity_threshold: 0.95
  similarity_fn: cosine
  use_existing_vector_db: false
  save_index: true
  num_gpus: 0
  vector_db_index_save_path: s3://autogluon-rag-github-dev/vectorDB_index_save/faiss/index.idx
  metadata_index_save_path: s3://autogluon-rag-github-dev/vectorDB_metadata_save/faiss/metadata.json
  vector_db_index_load_path: s3://autogluon-rag-github-dev/vectorDB_index_save/faiss/index.idx
  metadata_index_load_path: s3://autogluon-rag-github-dev/vectorDB_metadata_save/faiss/metadata.json

retriever:
  retriever_top_k: 20
  use_reranker: true
  reranker_top_k: 10
  reranker_model_name: BAAI/bge-large-en
  reranker_batch_size: 32
  num_gpus: 0
  reranker_hf_tokenizer_params:
    padding: true
    truncation: true
    max_length: 512
    return_tensors: pt

generator:
  generator_model_name: mistral.mistral-7b-instruct-v0:2
  use_bedrock: true
  bedrock_aws_region: us-west-2
  bedrock_generate_params:
    max_tokens: 512
  generator_hf_generate_params:
    max_new_tokens: 1024
  generator_query_prefix: You are a helpful chat assistant. You will be provided with a query and you must answer it to the best of your abilities. You will be provided with some additional context that is related to the query and will help you answer the question.
