import logging
from typing import Any, Dict, List, Union

import numpy as np
import pandas as pd
import torch
from torch.nn import DataParallel
from tqdm import tqdm
from transformers import AutoModel, AutoTokenizer

from agrag.constants import DOC_TEXT_KEY, EMBEDDING_KEY
from agrag.modules.embedding.utils import normalize_embedding, pool

logger = logging.getLogger("rag-logger")


class EmbeddingModule:
    """
    A class used to generate embeddings for text data from documents.

    Attributes:
    ----------
    hf_model : str, optional
        The name of the Huggingface model to use for generating embeddings (default is "BAAI/bge-large-en").
    pooling_strategy : str, optional
        The strategy to use for pooling embeddings. Options are 'mean', 'max', 'cls' (default is None).
    normalize_embeddings: bool, optional
        Whether to normalize the embeddings generated by the Embedding model. Default is `False`.
    hf_model_params : dict, optional
        Additional parameters to pass to the Huggingface model's `from_pretrained` initializer method.
    hf_tokenizer_init_params : dict, optional
        Additional parameters to pass to the Huggingface tokenizer's `from_pretrained` initializer method.
    hf_tokenizer_params : dict, optional
        Additional parameters to pass to the `tokenizer` method for the Huggingface model.
    hf_forward_params : dict, optional
        Additional parameters to pass to the Huggingface model's `forward` method.
    normalization_params: dict, optional
        Additional parameters to pass to the PyTorch `nn.functional.normalize` method.
    query_instruction_for_retrieval: str, optional
        Instruction for query when using embedding model.

    Methods:
    -------
    encode(data: List[str]) -> List[torch.Tensor]:
        Generates embeddings for a list of text data chunks.
    """

    def __init__(
        self,
        hf_model: str = "BAAI/bge-large-en",
        pooling_strategy: str = None,
        normalize_embeddings: bool = False,
        hf_model_params: Dict[str, Any] = None,
        hf_tokenizer_init_params: Dict[str, Any] = None,
        hf_tokenizer_params: Dict[str, Any] = None,
        hf_forward_params: Dict[str, Any] = None,
        normalization_params: Dict[str, Any] = None,
        query_instruction_for_retrieval: str = None,
    ):
        self.hf_model = hf_model
        self.normalize_embeddings = normalize_embeddings
        self.hf_model_params = hf_model_params or {}
        self.hf_tokenizer_init_params = hf_tokenizer_init_params or {}
        self.hf_tokenizer_params = hf_tokenizer_params or {}
        self.hf_forward_params = hf_forward_params or {}
        self.normalization_params = normalization_params or {}
        self.query_instruction_for_retrieval = query_instruction_for_retrieval
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        logger.info(f"Using Huggingface Model {self.hf_model} for Embedding Module")
        self.tokenizer = AutoTokenizer.from_pretrained(self.hf_model, **self.hf_tokenizer_init_params)
        self.model = AutoModel.from_pretrained(self.hf_model, **self.hf_model_params)
        self.num_gpus = torch.cuda.device_count()
        if self.num_gpus > 1:
            logger.info(f"Using {self.num_gpus} GPUs")
            self.model = DataParallel(self.model)
        self.model.to(self.device)
        self.pooling_strategy = pooling_strategy

    def encode(self, data: pd.DataFrame, pbar: tqdm = None, batch_size: int = 32) -> pd.DataFrame:
        """
        Generates embeddings for a list of text data chunks in batches.

        Parameters:
        ----------
        data : pd.DataFrame
            A table of text data chunks to generate embeddings for.
        pbar : tqdm, optional
            A tqdm progress bar to show progress.
        batch_size : int, optional
            The batch size to use for encoding (default is 32).

        Returns:
        -------
        pd.DataFrame
            The input DataFrame with an additional column for the embeddings.

        Example:
        --------
        data = pd.DataFrame({DOC_TEXT_KEY: ["This is a test sentence.", "This is another test sentence."]})
        embeddings = encode(data)
        """

        texts = data[DOC_TEXT_KEY].tolist()
        all_embeddings = []

        logger.info(f"Using batch size {batch_size}")

        batch_num = 1

        for i in range(0, len(texts), batch_size):
            logger.info(f"Batch {batch_num}")

            logger.info("\nTokenizing text chunks")
            batch_texts = texts[i : i + batch_size]
            inputs = self.tokenizer(batch_texts, return_tensors="pt", **self.hf_tokenizer_params)

            logger.info("\nGenerating embeddings")
            with torch.no_grad():
                outputs = self.model(**inputs, **self.hf_forward_params)

            logger.info("\nProcessing embeddings")

            # The first element in the tuple returned by the model is the embeddings generated
            # The tuple elements are (embeddings, hidden_states, past_key_values, attentions, cross_attentions)
            batch_embeddings = outputs[0]

            batch_embeddings = pool(batch_embeddings, self.pooling_strategy)
            if self.normalize_embeddings:
                batch_embeddings = normalize_embedding(batch_embeddings, **self.normalization_params)

            all_embeddings.extend(batch_embeddings.cpu().numpy())

            if pbar is not None:
                pbar.update(len(batch_texts))

            batch_num += 1

        if pbar is not None:
            pbar.close()

        data[EMBEDDING_KEY] = all_embeddings

        return data

    def encode_queries(self, queries: Union[List[str], str]) -> np.ndarray:
        """
        Function is used as written in: https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/flag_models.py
        This function will be used for retrieval task
        if there is a instruction for queries, we will add it to the query text

        Parameters:
        ----------
        data : List[str]
            A list of queries to generate embeddings for.

        Returns:
        -------
        Union[List[torch.Tensor], torch.Tensor]
            A list of embeddings corresponding to the input queries if pooling_strategy is 'none',
            otherwise a single tensor with the pooled embeddings.
        """
        if self.query_instruction_for_retrieval is not None:
            if isinstance(queries, str):
                input_texts = self.query_instruction_for_retrieval + queries
            else:
                input_texts = ["{}{}".format(self.query_instruction_for_retrieval, q) for q in queries]
        else:
            input_texts = queries
        return self.encode(input_texts)
