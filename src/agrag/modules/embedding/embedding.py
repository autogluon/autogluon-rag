import logging
from typing import List, Union

import numpy as np
import pandas as pd
import torch
from torch.nn import DataParallel
from tqdm import tqdm
from transformers import AutoModel, AutoTokenizer

from agrag.constants import DOC_TEXT_KEY, EMBEDDING_DIM_KEY, EMBEDDING_KEY
from agrag.modules.embedding.utils import normalize_embedding, pool

logger = logging.getLogger("rag-logger")


class EmbeddingModule:
    """
    A class used to generate embeddings for text data from documents.

    Attributes:
    ----------
    hf_model : str
        The name of the Huggingface model to use for generating embeddings (default is "BAAI/bge-large-en").
    pooling_strategy : str
        The strategy to use for pooling embeddings. Options are 'mean', 'max', 'cls' (default is None).
    normalize_embeddings: bool
        Whether to normalize the embeddings generated by the Embedding model. Default is `False`.
    hf_model_params : dict
        Additional parameters to pass to the Huggingface model's `from_pretrained` initializer method.
    hf_tokenizer_init_params : dict
        Additional parameters to pass to the Huggingface tokenizer's `from_pretrained` initializer method.
    hf_tokenizer_params : dict
        Additional parameters to pass to the `tokenizer` method for the Huggingface model.
    hf_forward_params : dict
        Additional parameters to pass to the Huggingface model's `forward` method.
    normalization_params: dict
        Additional parameters to pass to the PyTorch `nn.functional.normalize` method.
    query_instruction_for_retrieval: str
        Instruction for query when using embedding model.

    Methods:
    -------
    encode(data: pd.DataFrame, batch_size: int = 32) -> pd.DataFrame:
        Generates embeddings for a list of text data chunks in batches.

    encode_queries(queries: Union[List[str], str]) -> np.ndarray:
        Generates embeddings for a list of queries.
    """

    def __init__(
        self,
        hf_model: str = "BAAI/bge-large-en",
        pooling_strategy: str = None,
        normalize_embeddings: bool = False,
        **kwargs,
    ):
        self.hf_model = hf_model
        self.pooling_strategy = pooling_strategy
        self.normalize_embeddings = normalize_embeddings
        self.hf_model_params = kwargs.get("hf_model_params", {})
        self.hf_tokenizer_init_params = kwargs.get("hf_tokenizer_init_params", {})
        self.hf_tokenizer_params = kwargs.get("hf_tokenizer_params", {})
        self.hf_forward_params = kwargs.get("hf_forward_params", {})
        self.normalization_params = kwargs.get("normalization_params", {})
        self.query_instruction_for_retrieval = kwargs.get("query_instruction_for_retrieval", None)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        logger.info(f"Using Huggingface Model {self.hf_model} for Embedding Module")
        self.tokenizer = AutoTokenizer.from_pretrained(self.hf_model, **self.hf_tokenizer_init_params)
        self.model = AutoModel.from_pretrained(self.hf_model, **self.hf_model_params)
        self.num_gpus = torch.cuda.device_count()
        if self.num_gpus > 1:
            logger.info(f"Using {self.num_gpus} GPUs")
            self.model = DataParallel(self.model)
        self.model.to(self.device)

    def encode(self, data: pd.DataFrame, pbar: tqdm = None, batch_size: int = 32) -> pd.DataFrame:
        """
        Generates embeddings for a list of text data chunks in batches.

        Parameters:
        ----------
        data : pd.DataFrame
            A table of text data chunks to generate embeddings for.
        pbar : tqdm
            A tqdm progress bar to show progress.
        batch_size : int
            The batch size to use for encoding (default is 32).

        Returns:
        -------
        pd.DataFrame
            The input DataFrame with an additional column for the embeddings.

        Example:
        --------
        data = pd.DataFrame({DOC_TEXT_KEY: ["This is a test sentence.", "This is another test sentence."]})
        embeddings = encode(data)
        """

        texts = data[DOC_TEXT_KEY].tolist()
        all_embeddings = []
        all_embeddings_dim = []

        logger.info(f"Using batch size {batch_size}")

        batch_num = 1

        for i in range(0, len(texts), batch_size):
            logger.info(f"Batch {batch_num}")

            logger.info("\nTokenizing text chunks")
            batch_texts = texts[i : i + batch_size]
            inputs = self.tokenizer(batch_texts, return_tensors="pt", **self.hf_tokenizer_params)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            logger.info("\nGenerating embeddings")
            with torch.no_grad():
                outputs = self.model(**inputs, **self.hf_forward_params)

            logger.info("\nProcessing embeddings")

            # The first element in the tuple returned by the model is the embeddings generated
            # The tuple elements are (embeddings, hidden_states, past_key_values, attentions, cross_attentions)
            batch_embeddings = outputs[0]

            batch_embeddings = pool(batch_embeddings, self.pooling_strategy)
            if self.normalize_embeddings:
                batch_embeddings = normalize_embedding(batch_embeddings, **self.normalization_params)

            batch_embeddings = batch_embeddings.cpu().numpy()
            all_embeddings.extend(batch_embeddings)
            all_embeddings_dim.extend([batch_embeddings.shape[-1]] * batch_embeddings.shape[0])

            if pbar is not None:
                pbar.update(len(batch_texts))

            batch_num += 1

        if pbar is not None:
            pbar.close()

        data[EMBEDDING_KEY] = all_embeddings
        data[EMBEDDING_DIM_KEY] = all_embeddings_dim

        return data

    def encode_queries(self, queries: Union[List[str], str]) -> np.ndarray:
        """
        Function is used as written in: https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/flag_models.py
        This function will be used for retrieval task
        if there is a instruction for queries, we will add it to the query text

        Parameters:
        ----------
        queries : Union[List[str], str]
            A list of queries to generate embeddings for.

        Returns:
        -------
        Union[List[torch.Tensor], torch.Tensor]
            A list of embeddings corresponding to the input queries if pooling_strategy is 'none',
            otherwise a single tensor with the pooled embeddings.
        """
        if self.query_instruction_for_retrieval is not None:
            if isinstance(queries, str):
                input_texts = self.query_instruction_for_retrieval + queries
            else:
                input_texts = ["{}{}".format(self.query_instruction_for_retrieval, q) for q in queries]
        else:
            input_texts = queries
        return self.encode(pd.DataFrame({DOC_TEXT_KEY: input_texts}))
