import json
import logging
from typing import List, Union

import boto3
import numpy as np
import pandas as pd
import torch
from torch.nn import DataParallel
from tqdm import tqdm
from transformers import AutoModel, AutoTokenizer

from agrag.constants import DOC_TEXT_KEY, EMBEDDING_HIDDEN_DIM_KEY, EMBEDDING_KEY, LOGGER_NAME
from agrag.modules.embedding.utils import get_embeddings_bedrock, normalize_embedding, pool

logger = logging.getLogger(LOGGER_NAME)


class EmbeddingModule:
    """
    A class used to generate embeddings for text data from documents.

    Attributes:
    ----------
    model_name : str
        The name of the Embedding model to use for generating embeddings (default is "BAAI/bge-large-en" from Huggingface).
    model_platform: str
        The name of the platform where the model is hosted. Currently only Huggingface ("huggingface") and Bedrock ("bedrock") models are supported.
    platform_args: dict
        Additional platform-specific parameters to use when initializing the model, generating embeddings, etc.
    pooling_strategy : str
        The strategy to use for pooling embeddings. Options are 'mean', 'max', 'cls' (default is None).
    normalize_embeddings: bool
        Whether to normalize the embeddings generated by the Embedding model. Default is `False`.
    normalization_params: dict
        Additional parameters to pass to the PyTorch `nn.functional.normalize` method.
    query_instruction_for_retrieval: str
        Instruction for query when using embedding model.
    bedrock_aws_region: str
        AWS region where the model is hosted on Bedrock.

    Methods:
    -------
    encode(data: pd.DataFrame, batch_size: int = 32) -> pd.DataFrame:
        Generates embeddings for a list of text data chunks in batches.

    encode_queries(queries: Union[List[str], str]) -> np.ndarray:
        Generates embeddings for a list of queries.
    """

    def __init__(
        self,
        model_name: str = "BAAI/bge-large-en",
        model_platform: str = "huggingface",
        platform_args: dict = {},
        **kwargs,
    ):
        self.model_name = model_name
        self.model_platform = model_platform
        self.platform_args = platform_args

        self.pooling_strategy = kwargs.get("pooling_strategy", None)
        self.normalize_embeddings = kwargs.get("normalize_embeddings", False)
        self.normalization_params = kwargs.get("normalization_params", {})
        self.query_instruction_for_retrieval = kwargs.get("query_instruction_for_retrieval", None)
        self.num_gpus = kwargs.get("num_gpus", 0)
        self.device = "cpu" if not self.num_gpus else torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.embedding_model_max_tokens = self.platform_args.get("max_tokens")
        if self.model_platform == "bedrock":
            if not "embed" in self.model_name:
                raise ValueError(
                    f"Invalid model_id {self.model_name}. Must use an embedding model from Bedrock. The model_id should contain 'embed'."
                )
            logger.info(f"Using Bedrock Model {self.model_name} for Embedding Module")
            self.bedrock_embedding_params = self.platform_args.get("bedrock_embedding_params", {})
            if "cohere" in self.model_name:
                self.bedrock_embedding_params["input_type"] = "search_document"
            self.client = boto3.client(
                "bedrock-runtime", region_name=self.platform_args.get("bedrock_aws_region", None)
            )
        elif self.model_platform == "huggingface":
            logger.info(f"Using Huggingface Model {self.model_name} for Embedding Module")
            self.hf_model_params = self.platform_args.get("hf_model_params", {})
            self.hf_tokenizer_init_params = self.platform_args.get("hf_tokenizer_init_params", {})
            self.hf_tokenizer_params = self.platform_args.get("hf_tokenizer_params", {})
            self.hf_forward_params = self.platform_args.get("hf_forward_params", {})
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, **self.hf_tokenizer_init_params)
            self.model = AutoModel.from_pretrained(self.model_name, **self.hf_model_params)
            if self.num_gpus > 1:
                logger.info(f"Using {self.num_gpus} GPUs")
                self.model = DataParallel(self.model)
            self.model.to(self.device)
        else:
            raise NotImplementedError(f"Unsupported platform type: {model_platform}")

    def encode(self, data: pd.DataFrame, pbar: tqdm = None, batch_size: int = 32) -> pd.DataFrame:
        """
        Generates embeddings for a list of text data chunks in batches.

        Parameters:
        ----------
        data : pd.DataFrame
            A table of text data chunks to generate embeddings for.
        pbar : tqdm
            A tqdm progress bar to show progress.
        batch_size : int
            The batch size to use for encoding (default is 32).

        Returns:
        -------
        pd.DataFrame
            The input DataFrame with an additional column for the embeddings.

        Example:
        --------
        data = pd.DataFrame({DOC_TEXT_KEY: ["This is a test sentence.", "This is another test sentence."]})
        embeddings = encode(data)
        """

        texts = data[DOC_TEXT_KEY].tolist()
        all_embeddings = []
        all_embeddings_hidden_dim = []

        logger.info(f"Using batch size {batch_size}")

        batch_num = 1

        for i in range(0, len(texts), batch_size):
            logger.info(f"Embedding Batch {batch_num}")

            logger.info("\nTokenizing text chunks")
            batch_texts = texts[i : i + batch_size]
            if self.embedding_model_max_tokens:
                logger.info(f"\nTruncating Text chunks to size {self.embedding_model_max_tokens}")
                batch_texts = [text[: self.embedding_model_max_tokens] for text in batch_texts]

            logger.info("\nGenerating embeddings")
            if self.model_platform == "bedrock":
                batch_embeddings = get_embeddings_bedrock(
                    batch_texts=batch_texts,
                    client=self.client,
                    model_id=self.model_name,
                    embedding_params=self.bedrock_embedding_params,
                )
            else:
                inputs = self.tokenizer(batch_texts, return_tensors="pt", **self.hf_tokenizer_params)
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                with torch.no_grad():
                    outputs = self.model(**inputs, **self.hf_forward_params)

                # The first element in the tuple returned by the model is the embeddings generated
                # The tuple elements are (embeddings, hidden_states, past_key_values, attentions, cross_attentions)
                batch_embeddings = outputs[0]

            logger.info("\nProcessing embeddings")

            batch_embeddings = pool(batch_embeddings, self.pooling_strategy)
            if self.normalize_embeddings:
                batch_embeddings = normalize_embedding(batch_embeddings, **self.normalization_params)

            if isinstance(batch_embeddings, torch.Tensor):
                batch_embeddings = batch_embeddings.cpu().numpy()
            else:
                batch_embeddings = np.array(batch_embeddings)
            all_embeddings.extend(batch_embeddings)
            all_embeddings_hidden_dim.extend([batch_embeddings.shape[-1]] * batch_embeddings.shape[0])

            if pbar is not None:
                pbar.update(len(batch_texts))

            batch_num += 1

        if pbar is not None:
            pbar.close()

        data[EMBEDDING_KEY] = all_embeddings
        data[EMBEDDING_HIDDEN_DIM_KEY] = all_embeddings_hidden_dim

        return data

    def encode_queries(self, queries: Union[List[str], str]) -> np.ndarray:
        """
        Function is used as written in: https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/flag_models.py
        This function will be used for retrieval task
        if there is a instruction for queries, we will add it to the query text

        Parameters:
        ----------
        queries : Union[List[str], str]
            A list of queries to generate embeddings for.

        Returns:
        -------
        Union[List[torch.Tensor], torch.Tensor]
            A list of embeddings corresponding to the input queries if pooling_strategy is 'none',
            otherwise a single tensor with the pooled embeddings.
        """
        if self.query_instruction_for_retrieval is not None:
            if isinstance(queries, str):
                input_texts = self.query_instruction_for_retrieval + queries
            else:
                input_texts = ["{}{}".format(self.query_instruction_for_retrieval, q) for q in queries]
        else:
            input_texts = queries
        return self.encode(pd.DataFrame({DOC_TEXT_KEY: input_texts}))
