from typing import List

import torch
from torch.nn import functional as F


def pool(embeddings: List[torch.Tensor], pooling_strategy: str) -> List[torch.Tensor]:
    """
    Applies the specified pooling strategy to the embeddings.
    The pooling strategies supported are:
    - 'mean': Mean pooling of token embeddings.
    - 'max': Max pooling of token embeddings.
    - 'cls': Using the embedding of the CLS token.
    - None: No pooling, the input embeddings are returned as is.

    Parameters:
    ----------
    embeddings : List[torch.Tensor]
        A list of token embeddings generated by the Huggingface model. Each element in the list is a tensor of shape
        [batch_size, sequence_length, hidden_size].

    Returns:
    -------
    torch.Tensor
        A tensor of pooled embeddings according to the specified strategy. The output shape depends on the pooling
        strategy:
        - 'mean', 'cls', and 'max': [batch_size, hidden_size]
        - None: [batch_size, sequence_length, hidden_size]

    Example:
    --------
    output = self.model(input)
    embedding = output.last_hidden_state
    embedding = pool(embedding, 'mean')
    """
    if pooling_strategy == "mean":
        embeddings = embeddings.mean(dim=1)
    elif pooling_strategy == "max":
        embeddings = embeddings.max(dim=1).values
    elif pooling_strategy == "cls":
        embeddings = embeddings[:, 0, :]
    elif pooling_strategy:
        raise NotImplementedError("Provided pooling strategy not implemented")
    return embeddings


def normalize_embedding(embeddings, args=None):
    """
    Normalizes the input tensor (embedding).

    This function normalizes the input embeddings along a specified dimension using the specified parameters.
    It wraps the `torch.nn.functional.normalize` function, which applies Lp normalization over a specified dimension.

    Parameters:
    ----------
    embeddings : torch.Tensor
        The input tensor containing the embeddings to be normalized.
    args : dict
        Additional arguments to be passed to `torch.nn.functional.normalize`. This can include:
        - p (float): The exponent value in the norm formulation. Default: 2.
        - dim (int): The dimension to reduce. Default: 1.
        - eps (float): A small value to avoid division by zero. Default: 1e-12.

    Returns:
    -------
    torch.Tensor
        A tensor containing the normalized embeddings.

    Example:
    --------
    embeddings = torch.rand(10, 100)
    args = {'p': 2, 'dim': 1, 'eps': 1e-12}
    normalized_embeddings = normalize(embeddings, args)
    """
    if not args:
        args = {"p": 2, "dim": 1, "eps": 1e-12}  # default values from PyTorch
    return F.normalize(embeddings, **args)
